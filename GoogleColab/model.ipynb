{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ML model and Dataset creation\n"
      ],
      "metadata": {
        "id": "A7hGBQ80HZXe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnowV6vqI0mM",
        "outputId": "b4384320-d8d9-4593-a1cc-84767b2c1300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated CSV file 'cloud_workload_data.csv' has been created with DeploymentCU column.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Defining sample data\n",
        "data = {\n",
        "    'Timestamp': pd.date_range(start='2023-01-01', periods=100, freq='D'),\n",
        "    'IOPS': np.random.randint(10000, 100000, size=100),\n",
        "    'Compute': np.random.choice(['2 vCPUs', '4 vCPUs', '8 vCPUs', '16 vCPUs'], size=100),\n",
        "    'NetworkReq': np.random.choice(['100 Mbps', '1 Gbps', '10 Gbps'], size=100),\n",
        "    'BackupDedupeRatio': np.random.uniform(1.0, 4.0, size=100),\n",
        "    'Storage': np.random.randint(100, 10000, size=100),  # In GB\n",
        "    'RAM': np.random.choice(['8 GB', '16 GB', '32 GB', '64 GB'], size=100),\n",
        "    'Latency': np.random.randint(1, 100, size=100),  # In milliseconds\n",
        "    'WorkloadType': np.random.choice(['Web Server', 'Database', 'File Storage', 'Application'], size=100),\n",
        "    # Adding 'DeploymentCU' as the target variable with example categories\n",
        "    'DeploymentCU': np.random.choice(['Small', 'Medium', 'Large', 'X-Large'], size=100),\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "df['Timestamp'] = df['Timestamp'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "# Save to CSV\n",
        "csv_file_path = 'cloud_workload_data.csv'\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "print(f\"Updated CSV file '{csv_file_path}' has been created with DeploymentCU column.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load Data\n",
        "df = pd.read_csv('/content/cloud_workload_data.csv')\n",
        "\n",
        "# Clean Data (as before)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Feature Engineering & Preprocessing\n",
        "categorical_features = ['Compute', 'NetworkReq', 'RAM', 'WorkloadType']  # Assuming these are your features\n",
        "numerical_features = ['IOPS', 'BackupDedupeRatio', 'Storage', 'Latency']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(), categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Split Data\n",
        "# Now, 'DeploymentCU' is our target variable\n",
        "X = df.drop(['DeploymentCU', 'Timestamp'], axis=1)  # Dropping 'Timestamp' since it's not used here.\n",
        "y = df['DeploymentCU']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Applying transformations and training a model\n",
        "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', RandomForestClassifier(random_state=42))])\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HigufyNI5zX",
        "outputId": "9decadb6-6fbf-41bf-8ab5-6db9747e2986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Large', 'Large', 'Large', 'Small', 'X-Large', 'X-Large', 'Large',\n",
              "       'Large', 'Small', 'Medium', 'Small', 'Small', 'Large', 'Large',\n",
              "       'X-Large', 'Small', 'Large', 'Small', 'Small', 'Small'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "# Save the model to a file named 'model.pkl'\n",
        "model_filename = '/content/model.pkl'\n",
        "dump(model, model_filename)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tHBiRFHLPf-",
        "outputId": "c3d9284e-a27f-4608-ce1a-fc1fe704caba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ztq4fQuOBhVK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}